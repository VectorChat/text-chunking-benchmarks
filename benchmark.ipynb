{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-Only Zero-Shot Q/A Chunking Benchmark\n",
    "\n",
    "Please refer [here](https://github.com/VectorChat/chunking_benchmarks) for the main repository of the benchmarks. The chunks that we generated are located in the \"Resultant Chunked Dataset\" folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to use RAG (Retrieval Augmented Generation) for this benchmark as it is a common use case after having chunked text. Each chunk becomes an embedding that can be used to retrieve relevant information from a knowledge base. The retrieved information can then be used to generate answers to questions about the chunk via an LLM completion.\n",
    "\n",
    "Here's the setup:\n",
    "\n",
    "1. We chunk the text using various methods    \n",
    "    - Unstructured IOs [partitioner](https://docs.unstructured.io/open-source/introduction/quick-start), with the semantic chunking strategy\n",
    "    - AI21's [text segmentation api](https://docs.ai21.com/docs/text-segmentation-api), via [`AI21SemanticTextSplitter`](https://python.langchain.com/v0.2/docs/integrations/document_transformers/ai21_semantic_text_splitter/)\n",
    "    - Our chunker based on our internal research\n",
    "2. We save to a Vector DB (i.e. [Pinecone](https://pinecone.io), [Astra](https://astra.datastax.com/)) for retrieval\n",
    "3. Generate questions using foundational LLM (GPT-4o, Claude Sonnet 3.5, etc.) from a piece of fiction introduced after LLama 3 training\n",
    "4. Run RAG question-answering for each chunking method, score results based on answer accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's some sample code to understand the basic idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from enum import Enum\n",
    "from typing import List, Dict\n",
    "\n",
    "# Define our types up here\n",
    "\n",
    "class ChunkingMethod(Enum):\n",
    "    UNSTRUCTURED = \"unstructured\"\n",
    "    AI21 = \"ai21\"\n",
    "    CUSTOM = \"custom\"\n",
    "\n",
    "class VectorDB(Enum):\n",
    "    PINECONE = \"pinecone\"\n",
    "    ASTRA = \"astra\"\n",
    "\n",
    "# Chunk the text based on some method\n",
    "async def chunk_text(text: str, method: ChunkingMethod) -> List[str]:    \n",
    "    return [\"chunk1\", \"chunk2\", \"chunk3\"]\n",
    "\n",
    "# Save the chunks to a vector database\n",
    "async def save_chunks_to_vector_db(chunks: List[str], db: VectorDB):    \n",
    "    print(f\"Saving {len(chunks)} chunks to {db.value}\")\n",
    "\n",
    "# Generate questions based on the input text \n",
    "async def generate_questions(text: str, num_questions: int) -> List[str]:    \n",
    "    return [\"Question 1?\", \"Question 2?\", \"Question 3?\"]\n",
    "\n",
    "# Do a top k search to retrieve relevant chunks\n",
    "async def retrieve_relevant_chunks(question: str, db: VectorDB, top_k = 3) -> List[str]:    \n",
    "    return [\"relevant chunk 1\", \"relevant chunk 2\"]\n",
    "\n",
    "# Generate an answer based on the question and context chunks\n",
    "async def generate_answer(question: str, context: List[str]) -> str:    \n",
    "    return \"Generated answer based on context\"\n",
    "\n",
    "# Evaluate the generated answer\n",
    "async def evaluate_answer(generated_answer: str, correct_answer: str) -> float:    \n",
    "    return 0.85 \n",
    "\n",
    "# Run the benchmark\n",
    "async def run_benchmark(text: str, chunking_methods: List[ChunkingMethod], vector_db: VectorDB):\n",
    "    results = {}\n",
    "\n",
    "    # Chunk text using different methods and save to vector DB\n",
    "    for method in chunking_methods:\n",
    "        chunks = await chunk_text(text, method)\n",
    "        await save_chunks_to_vector_db(chunks, vector_db)\n",
    "\n",
    "    # Generate questions\n",
    "    questions = await generate_questions(text, num_questions=10)\n",
    "\n",
    "    # Run RAG question-answering for each chunking method\n",
    "    for method in chunking_methods:\n",
    "        method_scores = []\n",
    "        for question in questions:\n",
    "            relevant_chunks = await retrieve_relevant_chunks(question, vector_db)\n",
    "            answer = await generate_answer(question, relevant_chunks)\n",
    "            score = await evaluate_answer(answer, \"correct_answer\")  # Placeholder for correct answer\n",
    "            method_scores.append(score)\n",
    "        \n",
    "        results[method.value] = sum(method_scores) / len(method_scores)\n",
    "\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Long piece of fiction text...\"\n",
    "chunking_methods = [ChunkingMethod.UNSTRUCTURED, ChunkingMethod.AI21, ChunkingMethod.CUSTOM]\n",
    "vector_db = VectorDB.ASTRA\n",
    "\n",
    "benchmark_results = await run_benchmark(text, chunking_methods, vector_db)\n",
    "\n",
    "print(\"Benchmark Results:\")\n",
    "for method, score in benchmark_results.items():\n",
    "    print(f\"{method}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's some results\n",
    "\n",
    "Here's some results we got after multiple runs of the benchmark, the \"Percent Correct\" column is the percentage of questions answered correctly by the RAG model (excluding questions where the LLM had an invalid response) during the benchmark phase. The \"Cost\" column is the cost of the chunking method based on the number of API calls made and the cost per call during the chunking phase.\n",
    "\n",
    "| Method | Percent Correct | Cost |\n",
    "|--------|-----------------|------|\n",
    "| unstructured | 51.05% | $0.01 |\n",
    "| ai21 | 54.01% | $0.0106 |\n",
    "| chunking.com| **60.51%** | **$0.00504** |\n",
    "\n",
    "The Unstructured IO serverless API call took about 10 pages at a cost of $0.001 per page. The AI21 API took 11 calls (with limit of 100k characters per call).\n",
    "\n",
    "#### We used the following params:\n",
    "\n",
    "| Param | Value |\n",
    "|-------|-------|\n",
    "| max_chunk_size | 300 |\n",
    "| chunk_size_chars | 1200 | \n",
    "| top_k | 3 |\n",
    "| embedding_model | text-embedding-3-small |\n",
    "\n",
    "The novel had ~250k tokens (via llama 3's tokenizer) and was chunked with a max chunk size of 300 tokens (~1200 characters). The cost is based on the service api call or on the OpenAI embedding api calls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
